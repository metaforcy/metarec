
(* TODO(feature):
  lokalisieren: Judgements-Ids sind Seriennummern! (sonst ggf Ueberlappung von Namen in verschiedenen Targets)
    und der Benutzer gibt immer head_terms statt judgement-ids an
    also braucht man Netze/Tabellen
      head_term -> judgement-id 
      appl. head_term -> judgement-id
      judgement-id -> matcher, maker, head_term, mode, Benutzername
    zusaetzlich
      Judgement-Graph
*)

(* TODO(refactor):
     * nurnoch ein allgemeines Wohlgeformtheitsjudgement wf :: prop => prop => prop
       mit 1 input, 1 output
       Wohlgeformtheit von Judgementapplikationen wird dann ueber
       angabe von Regeln fuer wf realisiert, die die Proposition synthetisieren.
       zB    wf (x => A) (EX i. A => univ(i))
             wf (easy x y) True
*)


  
(* TODO(feature)
     * Regel-Prioritaeten nicht numerisch angeben sondern ueber Constraints an
       Prioritaetengraph spezifizieren

   TODO(opt)
     * reine Netze statt Item-Netze nutzen damit auch einfuegen von
       Regeln effizient; aber fraglich ob forward-rules soviele Regeln
       generieren das das jemals lohnt
     * statt gctxts, alles nurnoch auf Beweiskontexten (ggf aux Kontexten von lthys)
       laufen lassen und Theorien temporaer in Beweiskontexte einbetten ???
       FAIL: weil wir ja die Deklaration-Infrastruktur nutzen wollen
       und man so staendig und nicht nur einmal diese Einbettung fahren muesste.
*)








(* TODO(opt):

     * Avoid use of Variable.dest_fixes in rel_local_fixes etc. and remember
       outer fixes and metarec fixes ourselves. Cf. comment in rel_local_frees.

     * Fakten als proof_packs statt thms, Fakten dann iA non-ground,
       Fakten-Propagierung auf proof_packs umstellen
       um add_assm_terms_internal stark zu vereinfachen.

     * Completely avoid rule-freshening by using rule-app-local environments for
       matching variables and *shallowly* normalizing instantiated
       matching variables away in all cases (which completely removes them
       because of ensured groundness of matching variables).
         Rule matching then uses 2 environments, namely the matching var
       env which is extended and used for normalizing the pattern term
       and the globally threaded env for opportunistic normalization
       of the object term. 
         If debugging=true, check that matching variables (unifvars in the original rule)
       are actually gone in assumptions, subcall inputs and outputs.
       This can only be the case if matching variable groundness has been unsafely
       overridden via MR_unchecked rules.
         Was matching with shared variables in pattern and object terms just
       a defensive move by me, not realizing that due to freshening of
       rules, current matching variables could never occur in object terms anyway?
       Not yet: occurs in output pattern matching in solve_prems due to full
       normalization of premise and in mp_match_prf.
       Include debugging check for now?
         Rule-freshening itself is not that expensive due to smallness of rule
       and ease of maxidx access but adding freshened matching variables to
       the globally threaded env quickly results in *huge* env badly impacting
       normalization performance.
       Delaying rule-freshening is currently problematic regarding variable
       distinctification because we want to avoid deep variable instantiations?

     * Use var_exact_net for *active* constraints that is normalized and its contained constraints
       simplified in constraint_gen_proc and exconstraint calls to enable fast constraint lookup
       and avoid exconstraint memoization etc. (var_exact_net content merge is trivial:
       just adopt some entry of the set of entries that coincide in their constraints)
       We then need an explicit flag that avoids all simplifications of instantiated constraints
       in order to avoid non-termination resulting from exconstraint calls during simplification
       of instantiated constraints, instead of temporarily marking all constraints as simplified
       during simplification of instantiated constraints.
         Managing a constraint stack that retains simplified constraints in parallel remains important
       to allow factorization of constraints in a context in old ones and newly generated ones
       after CHR applications and to adapt ConstraintTrace of constraints generated in constraint
       simplifications. But we just cons onto this stack and do not need further operations
       or absorption or normalization of it, delegating them to the var_exact_net instead.
         Upon simplification active constraints are removed from the var_exact_net and
       moved to a list instead. (currently we just add the simplification proof
       to simpd_constraints and set the activity status of the constraint
       to SimplifiedConstraint; note that ConstraintTraces of simplified constraints
       are never looked at and the management of their proofs is only necessary in
       order to discharge assumptions corresponding to them after the derivation is complete;
       the absorption of newly generated constraints with existing already *simplified* constraints
       in constraint_gen_proc remains important as long as we do not simplify constraints
       right during their generation in constraint_gen_proc)
         Check whether the exconstraint result consistency even under intermittent
       relevant instantiations that is a consequence of exconstraint memoization
       is actually useful (usages of exconstraint results would have to be
       modulo constrainat simplification to account for later instantiations enabling
       further constraint simplifications, at least in extreme cases?).


     * Normalisierungen sparen wo moeglich. V.a. in metarec Ableitung, aber
       nicht in constraint Vereinfachung ueber CHRs weil Constraints fast immer
       kleine Terme sind die viel instantiiert werden und wir var_exact_net
       sowieso immer ausnormalisieren.
       Genauer:
       * top_metastruct_norm Funktion die obere !! xs. As ==> try(J ins outs)
         Struktur fuer solve_prems normalisiert wobei ins, outs nicht
         normalisiert werden.
       * Annahmen muessen vor der Kontexterweiterungs-Verarbeitung explizit voll
         ausnormalisiert werden weil direkte und lokale Regeln als normalisiert
         angenommen werden, insbesondere in ihren input-pos-Patterns die beim
         matchen gegen die Regel auch nicht weiter normalisiert werden.
       * Netzlookup (genauer Item_Net2.retrieve_match) das nur inkrementell
         betaeta- und env-normalisiert soweit noetig.
         Ausserdem optionales inkrementelles Varfixing im lookup-term um
         exactrules Behandlung nachzuvollziehen.
       * Bei Premissen-Discharge(impI), -Vereinigung(mp), -Eigenvariable-Check(allI)
         in proof_packs Konstruktoren wirkt staendig Normalisierung extrem teuer.
         Stattdessen Normalisierungs-sparende Hilfssuchfunktionen nutzen,
         zB rekursives aconv_mod_norm nutzen das nur
         soviel head_norm durchfuehrt bis Ungleichheit festgestellt.
       * Unbedingt schnelle Varianten der proof_packs Konstruktoren anbieten die kein
         LCF-style cross-checking betreiben, was ja sowieso nur dem debugging dient!
         Proposition und Premissen werden dann richtig lazy verwaltet (also mit expliziten
         verzoegerten unions und removes in Premissenliste und verzoegerten allI-Abstraktionen
         in Proposition) und Normalisierung findet nur beim Abrufen der Premissen (bzw. wenn
         verzoegerter Premissen-Berechnungs-Term gross wird um speicherfreundlich zu sein)
         und bei Kopf-Teilen der Proposition statt.
         Abruf der Beweis-Premissen ist sowieso relativ selten:
         kommt vor in replay_prf, nach CHR Anwendungen zu implied_by proof Berechnung,
         a constraint simp cyclicity check in metarec_constraint_simp_internal,
         Berechnung der rekursiven simp_deps fuer constraints0_simping und
         assm_disch in entstandenen simp_prfs (einfach vermeidbar), 
         sonst nur in tracing.
         Abstraktions-verzoegerten Allquantor in Propositionen nutzen und in
         erweitertem head_norm/norm/top_metastruct_norm(!) wieder entfernen. Wesentlich wird dann bei
         strip_all entsprechendes head_norm durchzufuehren.
       * Outside LCF-style cross-checking, prop_of_proofp is only used if result is small:
         it is only used in rule propositions (esp. in metarec_worker),
         in CHR related implication calculations,
         discharge of simplified constraints in metarec_constraint_simp,
         registration of derivation conclusion for hidden_univlvl_discharge
         in metarec_with_failcont,
         absorption of new constraints with existing local rules in constraint_gen_proc.
           This means we could delay its normalization outside LCF-style cross-checking
         mode, only head_norming its outer structure on proof_pack constructors while just
         assuming the constructor applications are valid. Actually allE_prf, mp_prf
         suggest that head_norming is not even necessary because rules are always
         normalized and outer structure is created by allI, impI.
           This works without explicit substitutions if we do not want to normalize Term.betapply
         in allE_prf, because free-abstracting universal quantifiers can be eliminated with
         via Term_Subst.instantiate. We only need such a strange quantifier
         because the proposition of the discharged instantiated proof of the rule
         after a metarec_worker call is done is not normalized, because the freshened
         matching variables of the rule are not lifted over fixes for performance reasons.
       * Normalising judgement outputs and the proposition of the
         discharged instantiated proof of the rule after a metarec_worker call
         is complete might be important for performance if terms are not huge.
         Normalizing judgement outputs avoid the constant lookups for instantiated
         (freshened) matching variables.
         Normalizing the proposition of the discharged instantiated proof of the rule
         avoids having to normalize the proof proposition or use a free-abstracting
         universal quantifier in allI_prf.
       * In Regel merken ob die sekundaeren Input-Positionen des Judgements alle Patterns sind
         statt staendig zu pruefen.
       * Zusammenhaengende Folgen von AllEs, mps, impIs, AllIs buendeln statt einzeln zu folden?



     * zu Input term in primaerer Position einen maximal-gesharetem Term-DAG verwalten
       und parallel in ihn absteigen bei Anwendung von _strukturellen Regeln_
       (primaere Objekte in Regel-Premissen sind
       Subterme des primaeren Objekts der Regel-Konklusion oder Substitutionen 
       mit frischen Variablen davon; die sind vermutlich sehr haeufig, ist ja quasi
       die Definition von primaerem Objekt das die Regel strukturell darin ist)
       Ableitungen dann auch maximal sharen indem man den Term-DAG annotiert.
       Koennte sich gut lohnen fuer Wohlformnungschecks fuer Typannotationen weil
       die Typen ja oft sehr aehnlich sind

       Wichtig ist ggf auch Umstruktierungen (insbes rewrites) eines Terms in
       primaerer Position auf dem Term-DAG nachzuvollziehen, unter Beruecksichtung
       von sharing in der Regelanwendung. (ist das dann schon Skeleton-Opt?)

       Pure/term_sharing.ML stellt maximales sharing syntaktisch
       gleicher Subterme her.

    Einordnung: Term-DAG-Optimierung entspricht sharing bzgl Termstruktur,
       Skeleton Optimierung entspricht sharing in Regeln?
       Wird die Skeleton-Optimierung die ich unten beschreibe einfacher wenn
       man die Term-DAGs Optimierung in Betracht zieht: nurnoch Propagierung
       von Resultaeten ueber lokales Backtracking hinweg?

       Term-DAG/Hashconsing Optimierung auch fuer beschleunigte
       Gleichheitspruefungen interessant, v.a. bei Typsynthese elaborierter
       Terme wo keine Unifvar instantiiert werden.


    !! WICHTIG bei Term-DAG-Optimierung und Skeleton-Optimierung etc.
    ist das so optimiert ausgefuehrte Judgements wenigstens quasi-*pure* sind,
    d.h. das bei selbem Input unabhaengig vom Ausfuehrungszustand aequivalente
    (v.a. sinnvoll unifizierbarere) Outputs entstehen. Sollte selbst bei
    Elaboration der Fall sein. Schwierig automatisch zu pruefen.




     * skeleton optimierung des simplifiers durchfaedeln
       caching des Typisierungs-Theorems von Subtermen wenigstens innerhalb einer Ableitung
       (und Subableitungen im Rewriting)
       ueber skeletons die an Var-Positionen Referenzen die bestehenden
       Ableitungen fuer diesen Subterm tragen
       * vllt einfaacher als Skeleton-Optimierung f. sharing von Normalisierungen:
         Tabelle mit "dieser Term schon normal" (vorallem fuer Judgements!)
         kann man ueber simprocs f. Judgements vermutl in bestehenden
         simplifier integrieren
         Problematisch wg Fixing-Context?
       * vllt einfacher als Skeleton-Opt f. sharing von Ableitungen:
         discrimination tree als Ableitungs-Cache f (bestimmte?) Judgements verwalten
       * Skeletons nur in primaeren Judgement-Pos verwalten?
         Knoten enthalten dann durch Objekte an den anderen Pos indizierte
         Ableitungen fuer das aktuelle Objekt an primaere Pos
       * wichtig(!): Ableitungen haengen nicht nur von den Termen, sondern
         auch von den Annahmen! Im allg sogar von Annahmen die nicht ueber Subterme sind!
     * Thm-Kisten zumachen nach abgeschl. Inferenz mit Thm.name_derivation !!!
       (vllt nur wenn Term/Ableitung gross genug war?)
     * nurnoch normalisieren wenn nicht folgendes Schema Normalitaet fuer das
       vorliegende Regelsystem garantiert:
          (t1 t2) normal ==> t1 normal, t2 normal
          (% x. t) normal ==> t normal
       d.h. dann nurnoch normalisieren wenn man neue Terme (dann hauptsaechlich wohl Typen)
       kombiniert
     * wenn Patterns von extrem einfacher Struktur wie (c $ ?x), (c $ (% x. ?f x))
       sind und die Terme direkt draufpassen gleich das allgemeinste Matching von
       Hand konstruieren statt ueber Pattern.match zu gehen




     * Compiler von metarec Klauseln zu Isabelle/ML Funktionen
       von ctermen zu thmen
       * Ideen zum allgemeinen Fall
         * Compilerlauf geht immer auf gewissen Metarec-Klauselmengen
           wenn neue dazukommen (insbes lokale) werden die ueber
           Continuations vor/nachgeschaltet falls die Ueberlappungs-
           situation der Klauseln und ihre Priorisieerung das
           zulaesst. Ansonsten zumindest die Klauselauswahl
           dynamisch ausfuehren.
         * partielle Applikationen von Judgements werden zu Closures
           von Aufrufen an die zugehoerige kompilierte ML-Funktion
         * Netze und higher-order (decomposing) pattern matching wegcompilen
           indem man alle Klauseln fuer ein Judgement gemeinsam betrachtet
           und splitting-Tree fuer die Matchings erstellt
         * cterm-Primitive nutzen beim matchen von ctermen auf patterns
         * kein decomposen und maken von Judgementapplikationen mehr,
           die Komponenten werden direkt als cterm-Tupel mitgefuehrt
         * Annahmen an fixes (ohne Premissen) speziell verwalten (in Symtab?),
           nicht als Regeln auf die man matcht
       * Pragmatischer Spezialfall
         Judgements bei ihrer Definition als kompilierbar vermerkt und zu
         ihnen duerfen dann statisch keine Annahmen abgesetzt werden
         (fuer dynamische Annahmen fuehren wir zusaetzlichen dynamischen Check ein
         das das Judgement der Annahme nicht kompilierbar ist).
         * Zu kompilierbaren Judgements koennen dann nicht nur Regelmengen, sondern auch kompilierte Closures
           bzw. PolyML-Funktionsnamen im Netz hinterlegt werden, deren Nutzung dann
           Prioritaet ueber Anwendung einer Regel aus der Regelmenge haben. Wenn Closure
           zu Judgement hinterlegt ist kann Judgement nicht mehr um neue Regeln erweitert werden,
           bzw. wird die Closure invalidiert oder das Judgement (und die kompilierbaren
           Judgements die von ihm abhaengen) neu kompiliert.
         * Der haeufige Fall von Annahmenverwaltung in Typsystemen funktioniert
           wird kompilierbare Judgements also nicht mehr direkt, sondern muss
           ueber nicht-kompilierbare Hilfsjudgements realisiert werden.
           Deren Lookup ist also nicht kompiliert sondern dynamisch ueber das Regelnetz,
           genau wie wir es brauchen.
         * Bei der Kompilation wird eine Menge von Judgements angegeben die
           kompiliert werden soll, damit wechselseitige Rekursionen zwischen ihnen
           als wechselseitige PolyML-Funktionen kompiliert werden koennen.
         * Aufrufe an Judgements zu denen keine PolyML-Funktion vorliegt, v.a.
           auch Aufrufe von hoeherstufigen Argumenten, werden normal ueber
           solve_prems durchgefuehrt.
         * Aufrufe an synth_procs werden direkt kompiliert
         * Ueberlappungen der Regeln des Judgements werden ueber hoeherstufige
           Anti-Unifikation der var-distinctified Input-Patterns gefunden
           und zu Splitting-Tree aufgeloest.
           Bei vorliegender vollstaendiger Ueberlappung mehrerer Regeln
           (d.h. unifizierbarer Inputpatterns) wird dann gemaess
           Regelpriorisierung in die kompilierten Regeln gelaufen,
           ggf. mit (try .. handle local_fail => ..) fuer in try gewrappte aufrufe,
           wobei handle zur Abarbeitung and der weiteren kompilierten ueberlappenden
           Regeln mit niedrigerer Prioritaet fuehrt.
         * Splitting-Tree-Tiefe heuristisch optimieren? NP-Complete?
           "Immer die Regeln deren Inputpatterns mit dem groessten Anti-Unifikat ueberlappen im
           Splitting-Tree gegenueberstellen, weil das die groessten case-Patterns ergibt."
           Alternativ: wir realisieren direkt eine n-fache Antiunifikation und
           ueberlassen PolyML die weitere Optimierung paralleler case-Patterns.
           N-fache Antiunifikation: Partitioniere zu antiunifizierende Patterns
           gemaess ggf. vorliegendem Head-Const/Free und der Antiunifikationen ihrer Typen,
           gemaess ggf. vorliegender Abstraktion (Domain-Typ kann ignoriert werden weil
           Invariante ist das Typen der zu Antiunifizierenden Terme bereits antiunifiziert wurden),
           gemaess vorliegendem Variablen-Head (egal welcher).
           Antiunifikation von Typen entsprechend und Antiunifikation
           von Sorten ist einfach Sorts.union.
         * Splitting-Tree wird in case-Expression kompiliert.
           Wir verwalten die Bijektion zwischen Meta-Variablen im Splitting-Tree
           und PolyML-Variablen im generierten Code.
           Matching von t gegen Pattern-Variablen-Applikation (?x $ bound_args)
           fuehrt zur Generierung von
             let x = t |> fold_rev abstract_over(?) bound_args
           Matching von t gegen Non-Pattern-Variablen-Applikatinon (?x $ t2) in
           primaerer Position fuehrt zur Geneierung von
             case t of
               x $ ... => ...
           Matching gegen bereits belegte Pattern-Variablen-Applikationen
           fuehrt (in diesen Positionen) zum Check auf beta-eta-Gleichheit
           unter der aktuellen Substitution.
         * Forme Zuweisungen von PolyML-Variablen an PolyML-Variablen
           zur direkten Nutzung der PolyML-Variablen um, d.h. genauer
           Substituiere die PolyML-Variablenidentitaet bei direkten
           Meta-Var zu Meta-Var Zuweisungen.
         * Zwecks code-sharing fuehren wir fuer fuer alle kompilierten Regeln
           des Judgements eine lokale Funktion ein mit den instantiierten Variablen
           in den Input-Pattern als Argumenten.
         * Premissen werden zu subcalls kompiliert (ggf. zu solve_prems falls es
           sich um nicht-kompilierte Judgements handelt).
           Kontexterweiterungen (!! xs. assms ==> ..) werden wie in metarec.ML
           zu fixes, assumes, Erweiterungen des Regelnetzes und deren discharge
           kompiliert.
             Das Matching der im subcall erzeugten Terme gegen Output-Patterns wird zu
           case Expressions kompiliert, wie oben beim Splitting-tree,
           wobei der Kontext bei matching gegen Pattern-Variablen-Anwendungen
           abstrahiert wird.
           Failing Matches gegen Output-Patterns werden zu  raise local_fail
           kompiliert.
             Der Beweisterm der im subcall zurueckkommt wie in metarec.ML
           ueber fixes, assumes discharged, ggf. try-gewrapped.
         * Substitutionen fuer die globalen Unifikationsvariablen werden
           wie in metarec.ML verwaltet und an subcalls durchgereicht,
           allerdings in den kompilierten Matchings nicht um Bindings
           an die lokalen Variablen erweitert.
         * Der Gesamtbeweis wird zusammengesetzt, d.h. die angewandte Regel
           und die Premissenbeweise werden gemaess der Belegung der PolyML-Variablen
           in der aktuell berechneten Substitution instantiiert und dann mit
           Modus Ponens komponiert.
         * Wir koennen Invariante halten das die anfaenglichen Inputs
           und abschliessenden Outputs von kompilierten Regeln immer normalisiert sind.
           (Dazu ggf synthprocs anpassen, v.a. unify_proc)
           Dann ist nur bei Applikationen von Matching-Variablen
           Normalisierungsaufwand notwendig.
       * Moeglichst typesafe Code-Generierungs-Kombinatoren nutzen,
         cf. BER MetaOCaml Diskussionen von Oleg und meine Versuche
         in Template Haskell.
*)



(* TODO(features): 
    * debugging 
      * approximative dynamic non-termination and (co-)cycle detection

        Wenn eine Regel 20-50 Mal angewandt wurde, non-termination
        vermuten, goal neu aufsetzen (fuer moeglichst kleines non-termination bsp)
        und mit tabling und subgoal abstraction ausfuehren.
        * "Tabling": loggen welche Regeln schon auf welche Inputs angewandt
          wurden um Zyklen zu finden. Waere auch allgemeinere (adaptive) Optimierung
          als sharing-orientierte Skeleton-Lord-Optimization.
            Um Tabling effizient durchfuehren zu koennen sind vermutlich Netze
          notwendig die die Termindizierung lazy nur bis zur Indizierungs-Eindeutigkeit
          in Symtabs ausrollen und auch unter lambdas indizieren.
          Weiterhin ist die Indizierung in nicht-triv. Kontexten zu loesen,
          ggf. ueber performantes lifting von Indizierungen in erweiterten Kontexten,
          sonst fallen wir zur Selekton-Lord-Optimierung zurueck.
          Unifvars in Termindizierungen muessen verfrischt behandelt werden
          damit Instantiierungen von Unifvars nicht den Termindex unvorhersehbar
          invalidieren? (bei Pientka im Wesentlichen so weil ueber existential Vars
          abstrahiert wird)
          (cf. Pientka's thesis:
            (NB: Pientka nimmt bei der Beschr des Algorithmus implizit kanonische
             Terme, d.h. beta-eta-long normal form, mit lowered existential vars.
             Deshalb wird die Behandlung von Matching auf Abstraktionen im
             Termindex viel einfacher weil kein near-eta-Redex im Term-Index
             behandelt werden muss, was Larry dazu bewogen hat Abstraktionen
             direkt als Wildcards anzusehen. Fuer uns ist on-the-fly
             eta-expansion waehrend Indexmatching auch eine moegliche Loesung?
             Ggf. sehr wichtige Optimierung bei Substitution Trees (die nicht in Lit.
             erwaehnt wird?) koennte Indizierung der Child-Substitutionsverfeinerungen
             ueber Sequenz der ersten Symbole jeder instantiierten Variable sein
             um wenigstens nicht im hier haeufigen lokalen best-case linear die
             passende Verfeinerung suchen zu muessen, insbesondere von der Wurzel
             aus gemaess Judgement indizieren.)
          tabling fuer Twelf ueber higher-order substitution tree index der
          auf linearisierten Term-Indices arbeitet und deren Teilterme shared;
          Pientka's "linearization", i.e. the factorization of unification
          into a assignment of a linear clause head (all unifvar occurs only once
          applied to all available bounds) with later solving of
          residuated variable definitions seems, similiar to the filtering
          process from discrimination net matching where unifvar are wildcards,
          with the main difference being that net matching does not descend
          into lambdas)
        * "Subgoal abstraction": subgoals die zu gross erscheinen werden
          abstrahiert, d.h. ab einer gewissen Tiefe (vermutlich am besten an
          Positionen wo Unifikationsvariablen einer Regel ansetzen wuerden) werden
          statt den Subtermen die dort stehen einfach Unifikationsvariablen
          eingesetzt. Wenn dieses abstrahierte Subgoal dann mit einem seiner
          Subsubgoals (ich vermute das X wird waehrrenddessen als fixiert
          angesehen) unifiziert (ohne occurs check!) hat man einen generativen
          Co-Zyklus gefunden. Z.B. p(f(X)) -> p(f(f(X))), wobei p Praedikat und f
          Funktionssymbol sei und p(f(X)) ist das bereits abstrahierte Subgoal,
          das p(f(f(X)) als Subsubgoal erzeugt, was mit p(f(X)) unifiziert (ohne
          occurs check).

          Constraints wechselwirken natuerlich damit wenn
        man welche absetzt und Constraint-Lookups in den relevanten Regeln
        verwendet (weil dann haengt das Resultat der Regel nicht nur von den
        Inputs, sondern auch von den mittlerweile ggf. veraenderten Constraints ab).
          Gleiches gilt fuer so ein paar andere seiteneffektige Spezial-Subgoals
        die , aber die sind bisher zu unwichtig als das ich mir Sorgen mache.
          Mir ist noch nicht klar ob das die Pragmatik so einer Analyse
        beeinflusst, weil wenn ein Subgoal wieder als Subsubgoal auftritt wirkt
        das so oder so pathologisch auf mich, auch wenn veraenderte Constraints
        beim Loesen des Subsubgoal dann ggf. veraendertes und potentiell
        terminierendes Verhalten bedingen koennten.
        Ist dann halt nicht mehr ganz konservativ die Analyse sondern erwartet
        ein sinnvolles Regelsystem.


      * Tracking des Ursprungs (d.h. des dort aktuellen rule traces)
        von Unifikationsvariablen.
        Anzeige der Urspruenge und die relevanten Constraints der
        uninstantiieren Unifvars bei err_with_trace?


    * Meta-Applikationen nur auseinandernehmen wenn es keine Regel existiert
      bei dem der gleiche, rigide Head ueber ZF-Applikationen angewandt ist.
      Meta-Applikationen nur auseinandernehmen wenn der Head (bzgl Meta-Appl)
      des Objekts nicht die ZF-Applikation ist
      Alternativ ne ordentlich abgegrenzte Warnung ausgeben falls mans doch macht.
    * optional normales HO-pattern matching fuer Objekte in primaerer Position
      statt decomposing HO-pattern matching (interessant zB wenn man in
      ZF sowieso immer Markierungskonstanten hat koennte man auch wieder das
      eta-contrahieren anfangen weil ehh alles Patterns sind und man
      Syntax, insbesondere Lambdas, immer explizit auseinandernimmt)
    * delay-Marker an Objekten um Matching auf non-HO-Patterns explizit zu
      verzoegern. Variablen sind dann nicht-available bis zu spaeterem
      Match mit ihnen.
    * forward-Regeln aus Annahmen (oder allg nach add_rule?) inferieren
      (ueber det. Regelsystem?) um Locale-style forward-propagation zu kriegen
      * spezielles Registrierungsattribut dafuer, damit man sicher weiss ob
        die Premissen des Fakts "echt" sind oder nicht
      * einfachste Lsg: wenn neue Annahme registriert wird,
        einfach Forward-Regeln mit passender Major-Premisse und dann
        loesbaren Side-Conditions anwenden bis Fixpunkt erreicht.
        Wenn man fuer Terminations-Fehlermeldung nur bis zu ner maximale Tiefe
        gehen will muss man bedenken das Superklassen-Ketten schon mal Laenge 10 o.ae.
        haben koennen.
      * beachte: Major-Prem von Forward-Regeln auf Concl der gerade registrierten/gen.
        Regel matchen, nicht umgekehrt (approximieren durch Unifbarkeit in Discrimination-Nets?)
      * lokale Annahmen des Regelsystems unter forward-Regeln abschliessen ??
      * Backward-Regelgenerierung durch Anwendung von forward-Regeln auf Conclusionen
        gerade registrierten Backward-Regeln ("composing forward rules") i.A. sehr problematisch:
          neue Conclusion kann viel weniger Variablen enthalten, man
          muesste wieder raten
        Vermutlich nur machen wenn die gerade registrierte Backward-Regel
        keine "echten" Premissen hat (ausser Side-Conditions an Var in der Concl,
        die man ja insbes f. Typisierung braucht), d.h. sowas wie ne ground-Instanz ist.
        Macht man bei Locales ja auch nicht anders?
        * einfaches Kriterium das zu approximieren??: Premissen in der Backward-Regel
          die generiert wuerde haben Var die in der neuen Concl nicht vorkommen.
          Was ist mit Strengthening? Premissen die nur Variablen enthalten die nicht
          mehr vorkommen sind nutzlos, oder?
      !!* kann man sogar verlangen das die nicht-"echten" Premissen in der Judgementapplikation
          in der Conclusion internalisiert sind, dann also garkeine Premissen vorliegen
          und man mit normalen forward rules auf Faktenmengen arbeiten kann.
          Klingt fuer Typklassen sinnvoll.
      * Auch fuer transitiven Abschluss von Coercions interessant?
        Sowas wie Iteration um Cumulation (bis zu Grenze) von
             A refines B~                          a : A <= B~ => f   ?????????
          ------------------------------------   --------------------------------------------
             (a : A <= B~) =>^cl (id ` a : A)           (a : A <= C~) =>^cl (g o f) ` a : C
        QUARK!!! viel einfacher:
          simple Coercion g : B <= C erzeugt forward-rules fuer Transitivitaetsabschluss
              f : A <= B
           -------------------------------         entsprechend nach in der Domain vorschalten
             generalize (g o f : A <= C)
          oder direkter
                f : A <= B  &&&  g : B <= C
           ----------------------------------------
               generalize (g o f : A <= C)
      * wenn man locales dann nicht mehr braucht vllt besser Coq-style sections
        implementieren mit fuer mich passendem lambda-Lifting
        (insbes auch von Regeln, insbes Typklassen-Kram!)
    * Auch normalization by det. rule system als Option anbieten?
      * Waere extrem coole Lsg um selektiv eta-long und normalization by evaluation
        (via quotation) zu kriegen.
      * Ist dann wohl zusaetzlich zu beta (und eta?), aber keine
        computational rules?
      * man gibt dann wohl fuer jedes Judgement an wie es normalisiert
        werden soll mit passendem subject/type-reduction thm, zB
           A ~> A' ==> (a : A) == (a : A')
           (a:A) ~> (a':A) ==> (a : A) == (a' : A)
        und um Thme zu normalisieren die der Worker erzeugt
        steigt man mit Simplifier in die propositionale Struktur ab
        und nutzt fuer das rewriten von Judgements entsprechende
        cong-Regeln die eventuell det. Regelsysteme anwerfen
    * Lokale computational rules (nutze ich ja effektiv schon ueber die Interpretation
      der simp-synthproc des rewto judgements *)
    * Selektiv in Judgements doch eta-normalisieren und fuer matching
      von Objekten in diesen Judgements dann decomposing pattern matching
      module beta, eta (statt nur modulo beta) verwenden.
      Die meisten Judgements haben ja keine Probeme mit eta, nur sowas
      wie simpto das einzelne lambdas auseinandernimmt.
    * non-rigide Konklusionen in Regelpremissen (= goals in lambdaProlog) weniger
      konservativ behandeln
      * braucht dann hoeherstufiges mode-Checking von Regeln um availability
        (= groundness) weniger konservativ zu berechnen
        * insbesondere mode-Annahmen an Judgementvariablen in Regeln R die man 
          in anderen Regeln R', bei Anwendungen von Judgements (in Goalpos, dh in Premissenkonklusionen) 
          die auf Regelkonkl von R matchen, ueberprueft
        * wie propagieren sich modes bei Kombinationen von Judgements mit Kombinatoren, 
          oder sowas einfach verbieten? Ist in lambda-Prolog ja dadurch einfacher
          das exakt der Head von Goal-Atome die potentiellen Regeln bestimmt.
          Bei mir waere das komplizieter falls es computational rules auf die Kombinatoren gibt.
          Vermutlich analog zu lambdaProlog die Kombinatoren selbst als (hoeherstufig) gemodete
          Judgements mit entsprechenden kombinierenden Regeln ansehen, zB
             map :: [-[-,+], -, +]
          (Vereinfachung im Vgl zu lambdaProlog etc: eindeutiges moding)
        * braucht man auch ordentliches Moding von generierten Judgements?
          zB map gecurried ansehen als
             map :: [-[-,+], +[-,+]]
        * checking geht dann vermutlich konkret so:
          * zu available Variablen auch einen Mode verwalten (der moeglicherweise
            trivial ist) und wenn sie als Judgement benutzt werden, entsprechend checken
          * danach checken ob die Outputpositionen der Regel-Konklusionen 
            das entsprechende moding des Judgements erfuellen, d.h.:
            sind ground (bzgl available Variablen) und haben
            den richtigen Submode (triv wenn es selbst Variablen sind;
            ansonsten braucht man Mode-Kombinationsregeln fuer
            Judgement-Kombinatoren-Funktionen (zB map als Funktion :: [-,+] => [-,+]
            mit Regel  [| J x y ; (map J) xs ys ==> (map J) (Cons x xs) (Cons y ys))
            aber vermutlich sollte man sich auf Judgement-Kombinatoren-Judgements
            beschraenken, was ja keine echte Einschraenkung ist, sonst muesste
            man computational rules checken)
          * am coolsten waere natuerlich fuer mode checking selbst wieder ne
            metarec zu nutzen, wie bei rule-wellformedness.
          * Warum nicht sogar ein eigenstaendiges "Typkonzept" (Inputs -> Outputs)
            fuer Judgements statt nur modes? Das wuerde dann quasi die Zuordnung
            Judgement -> Wohlgeformtheits-Judgement
            als Typisierung des Judgements reifizieren
            * das VeriML Paper suggeriert das die Soft-Typen von Judgements die
              Semantik des Judgements beschreiben sollten, insbes bei
              Judgement-Parametern
            * Soft-Type-Checking von MetaRec-Klauseln geht dann wohl etwa so
                * nehme die Soft-Wohlgetyptheit der Inputs des Judgements der Konklusion an
                * checke (v.l.n.r.) das die Inputterme in Judgements in den Premissen (=Goals)
                  zu den Judgement-softtypen passen, nehme an das die Outputs entsprechende
                  Softtypen haben
                * checke das die Outputs in im Judgement der Konklusions unter diesen
                  ganzen Annahmen zum Softtyp des Judgements der Konklusion passen
            * was ich mir mal mit den Wohlgeformtheitsjudgements gedacht habe
              ist darin dann enthalten und wesentlich richtiger:
                die Metainduktion passiert nicht einfach von Premissen zur Konklusion.
                Man geht von intensionalen Annahmen ueber Inputs der Konklusion aus,
                mit denen man die der Wohlgeformtheit der Inputs der ersten Premisse zeigt.
                Dann wird die Wohlgeformtheit der Outputs (vor allem wenn dies Variablen
                sind die noch nicht aufgetaucht sind, ggf auf distinct bounds angewandt)
                der ersten Premisse angenommen (das ist die erste Induktionshypothese),
                die weiteren Premissen entsprechend analysiert und letztendlich die Wohlgeformtheit
                der Outputs der Konklusion gezeigt
            * IMPORTANT TODO:
              muessen die Softtype-Annahmen an die Inputs der Konklusion von Regeln tatsaechlich
              als Annahmen in der Regel vorkommen, oder reicht ihre intensionale Annahme und
              die Semantik des Judgements soll den Rest uebernehmen? Vermutlich letzteres?
              Falls diese Annahmen tatsaechlich in der Regel vorkommen sollen, wie verwaltet
              man effizient ihre Ableitung??
            * eine wesentliche Einsicht hierbei ist das das moding
              essential ist um Metarec-Klauseln zu soft-typen, also wenn man moding
              ueber metarec auf Judgementsofttypes simulieren will muss man es
              wechselseitig-rekursiv machen (falls das ueberhaupt seperat geht)
            * Implementierungsalternativen
              * Meta-Telescopes (dh dependent pairs) nutzen um Judgment-Typisierungen,
                ueber Input/Output Tupel (also bundled statt unbundled Darstellung)
                zu beschreiben:
                  pair :: [{}, {}] {}
                  Pair :: 'a => 'b => pair 'a 'b
                  PairSet :: ('a => prop) => ('a => 'b => prop) => pair 'a 'b => prop
                    :== (% A F p. !! x y. p == Pair x y ==>  A x &&& F x y)
                  [| x ::> A  ;   y ::> F x |] ==> Pair x y ::> PairSet A F
                  sPairSet :: ('a => prop) => ('b => prop) => pair 'a 'b => prop
                    :== (% A B. PairSet A (% _. B))  (infix "**")
                  uSet :: unit => prop ::= (% _. True)
                  judgement :: ('a => prop) => ('a => 'b => prop) => ('a => 'b => prop) => prop
                    :== (% A F J. !! x y. A x ==> J x y  ==> F x y)
                und als forward rules zum Verwalten von Annahmen an Inputs/Outputs:
                    Pair x y ::> PairSet A F  ==>  x ::> A  &&&  y ::> F x
                    J ins outs ::> judgement InT OutT  ==>  ins ::> InT  &&&  outs ::> OutT ins
                Nicht schlimm weil rein syntaktisch ohne comp rules axiomatisiert?
                proplists habe ich ja auch schon ...
                Dann haetten wir sowas wie
                  metamap A B ::> judgement (judgement A B ** list A) (% _. list B)
                Aber was wenn man A, B nicht als Teil des Judgements sondern
                auch als Inputs wahrnehmen will?
                  metamap ::> judgement (PairSet Univ (% A. PairSet Univ (% B.
                        judgement A B ** list A)))
                      (% ins. list (snd (snd ins)))
                   FFFFFFFAAAAAAAIIIIIIILLLLLLLLL: braucht comp rule fuer snd
                     (aber die will man nicht axiomatisieren) !!!!!!

                Ist bundled Darstellung schlimm? Eher nicht, man kann ja Syntax
                  metamap' J x y == metamap (Pair J x) y
                nutzen bzw sogar Definitions-Support der das ableitet anbieten.
                Das decompose_judgement, maker, matcher muss man halt anpassen
                und die Judgement-Terme sind etwas groesser.

              * fuer jede simple moding Variante ein eigenes Judgement-Typisierungs-Judgement
                bereitstellen (adaptiv automatisiert?) klingt lame





              * Judgement-Typen als Meta-dependent-Products mit Spezialannotationen
                   input, output :: 'a::{} => 'a  :== (% x. x)
                   judgement :: prop => prop  :== (% P. P)
                   jud :: 'a => 'a  jud :== (% x. x)
                     !! jud und judgement braucht man eigentlich nicht wenn
                     man konsequent immer AnyJud, TrueJud, BoolJud etc in der Codomain 
                     von Praedikaten nutzt die Judgements sein sollen !!
                   conjty :: ('a => prop) => ('a => prop) => ('a => prop)
                     :== (% P Q x. P x &&& Q x)
                   AnyJud :: prop => prop  :==  (% P. True)
                     mit  P ::> AnyJud
                   TrueJud :: 'a => prop  :==  (% x. True)
                   BoolJud :: prop => prop  :==  (% P.  METAEX p. P == Trueprop p)
                     wobei  METAEX :: ('a :: {} => prop) => prop
                              ::= (% P. !! Q::prop. (!! x::'a. P x ==> Q) ==> Q)
                   ConstrProp :: prop => (prop => prop)  :== (% P J. J ==> P)
                   ConstrBoolJud :: prop => (prop => prop)  :== (% P J. J ==> P  &&&  BoolJud J)
                   constraintedprop :: prop => prop => prop  :== (% P Q.  P &&& Q)
                   derive_constraint :: prop => prop => prop  :== (% P Q.  Q ==> P) 
                zB
                   metamap ::> judgement (gmPI i :> input univ,  A :> input Univ(i),  B :> input Univ(i),
                     J :> input (judgement (Pi x :> input A, y :> output B. BoolJud)),
                     xs :> input (list A),  ys :> output (list B).  BoolJud)
                ist aber recht haessliche Notation und man will den semantischen Teil
                so einer Judgement-Typisierung ja auch in der Judgement-Semantik haben,
                also will man eines von beiden inferieren. Vermutlich geht das sogar
                einfach ueber lokale Typinferenz wenn man entsprechende Meta-Lambdas nutzt.
                
                Ansatz unten alles in der Semantik als speziell-interpretierte Konjunktion/Implikation
                festzulegen klingt erstmal einfacher, allerdings ist dort die Charakterisierung von
                Judgement-Parametern unschoen und "semantisches Currying" ist nicht-trivial.
                Der Ansatz hier muss gebootstrappt werden mit den Soft-Typisierungs-Regeln
                fuer gmPi etc.


                FAIL mit gmPi:
                  dann haette der Ansatz hier hat null Semantik weil
                     J ::> (gmPi x :>i A, y :>o B. BoolJud)
                  immer gilt und insbes daraus nicht folgt das
                     x :> A  ==> (!! y.  J x y  ==>  y :> B)

                Loesung: statt gmPi muss man PiSigma Types nutzen um Outputs zu charakterisieren
                  (PiSig_n x1 A1 ... xn An. B x1 .. xn) :=
                    (% f. !! x1 .. xn. f x1 .. xn : B x1 .. xn ==> x1 : A1 &&& ... &&& xn : An)
                  das unten folgende muss man noch mutatis mutandis auf PiSig_n Typen umstellen

                Notation wird besser wenn man  statt input, output zu schreiben das Judgement
                :> annotiert und ich nutze => Notation statt gmPI Notation.
                   metamap ::> judgement (i :>i univlvl  =>  A :>i Univ i  =>  B :>i Univ i,
                       =>  J ::>i (judgement (x :>i A  =>  y :>o B  =>  BoolJud))
                       =>  xs :>i list A  =>   ys :>o list B  =>  BoolJud)
                     ::= jud
                      (% i :>i univ ,  A :>i Univ i ,  B:>i Univ i,
                         J ::>i judgement (x :>i A  =>  y :>o B  =>  BoolJud),
                         xs :>i list A,  ys :>o list B.
                           metamap_i (THE p. !! x y. J x y == Trueprop p) xs ys)
                     wobei metamap_i folgende induktive Def hat:
                       metamap_i p [] []
                       p x y  ==> metmap_i p xs ys ==> metamap_i p (Cons x xs) (Cons y ys)
                     fuer metamap kann man also unter der Premisse
                       J ::> judgement (x :>i A => y :>o B => BoolJud)
                     folgende Regeln zeigen
                       metamap J [] []
                       [|  J x y  ;  metamap J xs ys  |] ==> metamap J (Cons x xs) (Cons y ys)
                         
                   bottomuprew ::> judgement ( i :>i univlvl  =>  A :>i Univ
                       =>  rew ::>i  judgement (x :>i A  =>  y :>o A  =>  ConstrProp (Eq x y))
                       =>  t :>i A  =>  t' :>o A  =>  ConstrProp (Eq t t') )
                     ::= jud
                       (% i:> univlvl,  A :>i Univ,  rew ::>i (x :>i A  =>  y :>o A =>  ConstrProp (Eq x y)),
                          t :>i A,  t' :>o A.
                            constrainedprop (Eq t t') True)
                Etwas lame wird das wenn man nicht nur binaere infix-Judgements nutzt
                um Judgement Positionen zu charakterisieren. Auch etwas lame wenn man
                mehr als ein Judgement nutzt um eine Judgmentposition zu charakterisieren:
                man braucht eine extra Konjunktion.
                 

                Killer-Argument fuer eine solche echt-seperate Judgement-Typisierung ist
                das "semantisches Currying" sehr einfach zu beschreiben ist:
                     [|  J ::> judgement (gmPI x with J2 x. A(x)) ;  J2 e  |] ==>
                   J e ::> judgement (A(e))

                   Folgende Regel hat keine hohe prio um triviales Discharge zu ermoeglichen
                   wenn J ::> judgement (BoolJud) schon eine Annahme ist:
                     try(  J ::> judgement (Trueprop(p))  )  ==>  
                   J ::> judgement (BoolJud)

                     J ::> judgement (P)  ==>
                   J ::> judgement (AnyJud)

                     [|  J ::> judgement (P)  ;  derive_constraint P' P  |] ==>  
                   J ::> judgement (ConstrProp P')

                   derive_constraint' P' P  nutzt eine Taktik die Meta-Konjunktionen
                   in P auseinandernimmt (falls P die Form ConstrProp (... &&& ...) hat),
                   prueft das die atomaren Propositionen
                   Judgement-Applikationen sind, diese annimmt und dann die atomaren
                   Meta-Konjunkte von P' ueber metarec herleitet
                   (die auch alle Judgement-Applikationen sein muessen)
                   Entspricht refinement-Types?


                   damit gilt dann
                     metamap i A B ::> judgement (xs :>i list A  =>  ys :>o list B  =>  BoolJud)
                   man kann also ohne weiteres
                     metamap i (list A) (list B) (metamap i A B)
                       ::> judgement (mPi xss :>i list (list A)  =>  yss :>o list (list B)  =>  BoolJud)
                   bilden


                
                Anwendungsbeispiel: reflexive Invarianzableitungen in SetoidIsotrans checken
                Das klingt wesentlich nuetzlicher als das world-Checking der Unterwasserkreaturen
                Technologie

                  invariant ::> judgement ( AA :>i setoid  => t1 ::>i (conjty (set A) varsinv)
                        =>  t2 :>i (conjty (set A) varsinv) => AnyJud)
                      :== (% AA :>i Setoid,  t1 :>i A,  t2 :>i A.   eqOf AA t1 t2  )

                  Etwas bloed ist das wir eigentlich AA als Output haben wollen
                  der hinten stehen soll und primaere Objekte (t1, t2) sind auch noch
                  nicht charakterisiert!
                  Will man also vllt eher folgendes?
                    invariant' ::> judgement ( t1 ::>i varsinv  =>  t2 ::>i varsinv
                      =>  AA :>o (conjty setoid (conjty (contains t1) (contains t2))) =>  AnyJud )

                      wobei
                        contains :: judgement ( x ::> TrueJud  =>  J ::> judgement ( x ::> TrueJud => BoolJud )
                            => BoolJud )
                          :==  jud (% x, J. J x)
                      


                  varsinv ::> judgement ( t ::> TrueJud  =>  AnyJud  )
                    :==  (% t ::> TrueJud.  True)

                  frule:
                    invariant AA t1 t2  ==>  varsinv t1  &&&  varsinv t2
                    (beachte: die Subterme von t1, t2 sind damit nicht als varsinv
                    getaggt, aber das will man auch nicht)

                  triv-wf frule: :
                      [| invariant AA x x  ;  variable x |]  ==>  varsinv x
                    (wird ausgefuehrt nach assumen von Annahmen, aber vor ihrem wf-Check;
                    charakterisiert somit insbesondere Annahmen die triviale wohlgeformt sind)
                    variable ist ein extra-logisches Judgement, das prueft ob das Argument
                    eine Variable (konkret eine fixed Variable) ist.

                  brules:
                    [|  varsinv t1  ;   varsinv t2  |] ==> varsinv (t1 t2)
                    [| !! x.  varsinv x  ==>  varsinv (t x)  |] ==>  varsinv (% x. t x)

                  Die Wohlgeformtheit von Premissen (!! x. A ==> J) von brules
                  checkt man folgendermassen: fixiere x, checke A auf Wohlgeformtheit,
                  nehme A an und checke J auf Wohlgeformtheit.
                     






              * Judgement-Typen nicht als Objekte der Logik bereitstellen,
                sondern nur ein Mapping von Judgements auf ihre Komponenten-Typisierung
                (und moding) als spezielle Regeln (wofuer man dem User schoene Syntax anbietet):
                   (x : A |-> x' : A' via f) ==> input (x : A) &&& input (A : Univ)
                      &&& output (x' : A') &&& output (A' : Univ)
                   wobei  input, output :== (% P. P) 
                     * input (J x ...)  sagt aus das das primaere Objekt x 
                       in der Judgementapplikation (J x ...) in einer Inputposition steht
                       und eine Typisierung gemaess (J x ...) erhalten soll
                     * input (!! zs. J zs ==> input(J' z ...) &&& ...)  sagt das der
                       Judgementparameter J input ist und sein Judgement-Typ
                       ueber Sub-Judgements J'
                       auf seinen inputs/outputs gegeben ist
                   (vllt klarer als  input x (J x ...)  ??)

                higher-order Judgement Typen werden dann als geschachtelte Horn-Klauseln
                beschrieben:
                   metamap J xs ys ==> 
                     input (!! x y. J x y ==> input (x : A) &&& output (y : B))
                     &&& input (xs : list(A)) &&& output (ys : list(B))

                wenn man Teile der (intensionalen) Semantik von Judgement-Parametern
                charakterisieren will macht man das zusaetzlich als Konsequenzen in
                diese geschachtelten Horn-Klauseln:
                   bottomuprew A rew t t' ==> 
                     input (A : Univ) &&&
                     input (!! x y. rew x y ==> input (x : A) &&& output (y : A) &&& Eq x y)
                     &&& input (t : A) &&& output (t' : A)
                     &&& Eq t t'

                   wobei
                     bottomuprew A rew t t'  :==  A : Univ  &  t : A  &  t' : A  &  t = t'
                       &  (!! x y. rew x y ==> x : A & y : A & Eq x y)
                     Eq x y  :== x = y
                     [| x simpto t' ; y simpto t' |] ==> Eq x y

                   man kann dann in metarec-Klauseln fuer  bottomuprew A rew t t'
                   annehmen das rew Gleichheiten erzeugt und passende typisierte Terme
                   erzeugt:
                      [|  judassm (!! x y. rew x y ==> x : A & y : A & Eq x y)  ;
                          judassm (atom : A)  ;
                          judassm (A : Univ)  ;
                          rew atom t  ;  bottomuprew A rew t t'  |] ==>
                      bottomuprew A rew atom t'
                   Diese Zusatzannahmen in  judassm := (% P. P)  werden automatisch
                   discharged (und gecached). Man koennte auch ueberlegen ob man
                   sie nicht implizit laesst (dh nach dem Parsen der Proposition 
                   aus der Judgementtypisierung des Konklusionsjudgements generiert
                   und davorpackt) und als MetaRec-Annahmen zur Benutzung im Beweis
                   registriert.

                   wenn man jetzt
                     bottomuprew nat natrew t t' 
                   in einer MetaRec Klausel nutzt wird insbes
                     (!! x y. natrew x y ==> x : N  &  y : N  &  Eq x y)
                   automatisch hergeleit, wobei man die Deklaration des Judgement-Typs von
                   natrew als forward-rule nutzt:
                      natrew x y ==> input (x : N) &&& output (y : N) &&& Eq x y

                Ist es vllt interessant wenn man statt eine seperate Judgement-Typisierungs
                -Regel zu staten direkt die Semantik-Definition des Judgements entsprechend
                annotiert? Das gaebe dann zB
                   (x : A |-> x' : A' via f) == input x (x : A) &&& input A (A : Univ)
                      &&& output x' (x' : A') &&& output f (f : bij A A')
                      &&& check (Eq (f x) x')
                   metamap i A J xs ys == implinput i (i : univlvl)
                      &&& implinput A (A : Univ(i))
                      &&& input J (!! x y. J x y ==> input x (x : A) &&& output y (y : A))
                      &&& input xs (xs : list A)  &&&  output ys (ys : list A)
                      &&& (ALL (x,y) : xs ~~ ys.  J x y)
                   metamapcomb i A J J' == implinput i (i : univlvl)
                      &&& implinput A (A : Univ(i))
                      &&& input J (!! x y. J x y ==> input x (x : A) &&& output y (y : A))
                      &&& output J' (!! xs ys. J' xs ys ==>
                            input xs (xs : list A)  &&&  output ys (ys : list B)
                            &&& (ALL (x,y): xs ~~ ys. J x ))
                Sieht ganz gut aus. Judgementparameter darf man nicht mit == ueberspezifizieren.
                Etwas doof ist das alles prop-wertig ist, aber das ist ja bloss
                ein Isabelle-Problem das man mit Behandlung von boolinput, booloutput
                und & loesen kann.

                Jetzt will man natuerlich noch Support fuer die Nutzung von
                higher-order Judgementkombinatoren.
                ZB will man  metamap i A J  schreiben
                was die intensionale Semantik von  metamapcomb i A J  haben soll.
                Wenn man das hat kann man automatisch schachteln:
                  metamap i (list A) (metamap i A J) xxs yys
                Also braucht man sowas wie "semantisches currying".
                Vllt auch "over-application" wenn es einen Judgementwertigen Output
                gibt? Aber das kann man ja einfach reparieren durch maximale
                Judgementapplikation in der Def.
                Gecurried higher-order Terme hat man ja sowieso schon,
                nur die intensionale Semantik beim checken und die
                judassm  Annahmen muss ich anpassen.


                Will man vllt manchmal unterspezifizierte Judgements  J x y
                die x, y nicht per se mit ueber ihrer Semantik einschraenken?
                Vermutlich nicht so wichtig.
                Man muesste dann zu
                  J x y == input(x, x : A) ==> output (y, y : B)
                uebergehen und im Beweis von Metarec-Klauseln tatsaechlich
                die vorhandene  judassm (x : A)  nutzen.
                Klingt etwas sauberer. Wir haetten dann:
                   (x : A |-> x' : A' via f) == [|  input x (x : A) ;  input A (A : Univ) |]
                      ==> output x' (x' : A') &&& output f (f : bij A A')
                        &&& check (Eq (f x) x')
                   metamap i A J xs ys == [|
                        implinput i (i : univlvl)  ;
                        implinput A (A : Univ(i))  ;
                        input J (!! x y. J x y ==> input x (x : A) ==> output y (y : A)) ;
                        input xs (xs : list A)  |]
                      ==>  output ys (ys : list A)
                        &&& (ALL (x,y) : xs ~~ ys.  J x y)
                   metamapcomb i A J J' == [|
                        implinput i (i : univlvl)  ;
                        implinput A (A : Univ(i))  ;
                        input J (!! x y. J x y ==> input x (x : A) ==> output y (y : A))  |]
                      ==> output J' (!! xs ys. J' xs ys ==>
                            input xs (xs : list A)  ==> output ys (ys : list B)
                              &&& (ALL (x,y): xs ~~ ys. J x ))
                 Vllt kann man dann sogar auf annotation mit input/output verzichten
                 und alle Annahmen die Metarec Judgements sind zu inputs machen
                 und alle Konklusionen die Metarec Judgements sind zu outputs.


      * dependency analysis nutzt bereits das arbitrary-Judgement als
        dependency von Regeln wenn eine Judgment-Variable benutzt wird
      * exaktere hoeherstufige dependency analysis bei Anwendungen von Judgement-Kombinatoren:
        man betrachtet solche Kombinatoranwendungen als Knoten im dependency graph, zB
           Regel mit (map J xs) -> map J -> J    statt wie bisher   Regel -> map -> arbitrary
        Vermutlich einfach allgemein als
           Regel mit (F Js ts) -> F Js
           F Js -> J_i
           F Js -> J'   (falls J' konkret in einer Regel fuer F vorkommt)
        approximieren, egal was der Kombinator F tatsaechlich
        mit den J_i macht

        Das ist also eine Art Kontrollflussanalyse mit parametrischen Variablen,
        die ground-terms beachtet (ohne comp rules momentan).
        Vllt auch interessant fuer die CFA im "perfekten Editor"?
        
        Ich habe ja momentan keine anonymen Judgements (was Closures entsprechen wuerde),
        also ist eine Environment-Analyse unnoetig. Aber koennte man das kriegen
        indem man die Lambdas einfach syntaktisch abstrakt mitnimmt und beta-reduziert?
        Bzgl der Abhaengigkeiten werden Ausdruecke im Lambda, die Variablen
        enthalten die im aktuellen Kontext unbekannt sind, zu Wildcards.
        Fuer Compilerbauer moeglw interessant???
        Normalisierung mit comp. rules entspricht dabei constant
        folding bzw partial evaluation.



        Implementierung vermutlich mit "Pattern-Graphen":
          Knoten sind Patterns mit Variablen, Kanten entsprechen
          parametrischen Abhaengigkeiten und sind mit (partiellen) Instantiierungen
          der Variablen im Ausgangsknoten beschriftet die vorliegen
          muessen wenn die Abhaaengigkeit bestehen soll (konkret also
          das Pattern des Ausgangsknoten in dieser Instantiierung auf
          ein Pattern einer Regel matcht die eine Abhaengigkeit zum Zielknoten
          induziert).
          Laufen auf Pfaden komponiert die Instantiiergen.
          Wenn in einen Zyklus laeuft, hat man also ueber die Instantiirung
          direkt einen Term (mit Variablen; allgemeinst falls Zyklus zum Startknoten)
          auf alle Knoten matcht.

          Bspgraph:   metamap ?J ?xs *   -- ?xs := (Cons ?x' ?xs')  -->  metamap ?J ?xs' *
          Bsppfad:  metamap ?J ?xs *  -->  metamap ?J (Cons ?x' ?xs') *  -->  metamap ?J (Cons ?x' (Cons ?x'' ?xs'')) *
                hier muss Zykluserkennung sofort anschlagen!

          Vermutlich reichen mir first-order patterns um die
          Stratifizierung von Abhaengigkeiten mit Markern zu erkennen.
          Nicht-Triv Higher-Order Patterns werden dann als Wild-Cards
          interpretiert. Alle Terme matchen auf Wildcards und Wildcards
          matchen auf alle Patterns.
          Vllt fuer Performance auch interessant wenn man deklarieren
          kann das bestimmte Pos von Judgements immer als Wildcards
          approximiert werden, egal ob in der Regel dort nicht-triv
          higher-order Patterns vorliegen.
          Vermutlich sogar ratsam fuer alle Patterns (und die Ausdruecke
          die aus ihren Variablen gebaut werden) die keine Variablen
          und keine ground-terme sind sind Wildcards einzusetzen.
          Aber Applikationen von Marker-wertigen Funktionen werden
          nicht zu Wildcards -> Marker ueber Typ/Soft-Typ charakterisieren.
          Echte Behandlung von Higher-Order Patterns waere interessant
          fuer Regeln (zB Applikationregel fuer gmPi) die ein Judgement
          aus einem Term fischen.

          Konkret sind hier *alle* Judgement-*Applikations*-patterns die in brules
          und frules vorkommen die Knoten (modulo Umbenennung von Varianten) und die
          Kanten sind die entsprechenden Abhaengigkeiten die sich durch
          die brules und frules ergebenden.

          Muss man aufpassen das man die Outputs nicht zum Judgement-Applikations-Pattern
          rechnet? Waere wohl besser, aber wenn so eine Abhaengigkeit in
          der Ausfuehrung vorliegt, gaebe es dann ja einen Fehlschlag.
          Wenn man fuer die Outputs immer Wildcards einsetzt kann einem wohl
          garnichts passieren!

          Wenn Abhaengigkeiten entstehen die Variablen enthalten die
          im lhs Judgement-Applikations-Pattern der Regelkonklusion nicht vorkommen
          macht man diese Variablen und Ausdruecke die sie enthalten
          zu Wildcards.

        Man "weiss" also garnicht explizit welche Positionen eines Judgements
        zu seinem Dependency-Graph beitragen: ergibt sich dadurch welche andere
        Judgementapplikationen von den aktuell existierenden Regeln genutzt werden.
        Manche haben eben Judgementparameter als Kopf die in einer gewissen
        Position der Regelkonklusion stehen...
        zB
           J 2 y  ==>  App2 J y  (y output)
           erzeugt die Abhaengigkeit
             App2 ?J * -> ?J 2 *

           [|  J x y  ;  J2 y z  |]  ==> App3 J J2 x z  (z output)
           erzeugt die Abhaengigkeiten  
             App3 ?J ?J2 ?x *  ->  ?J ?x *,  ?J2 * *
           (genauer: erst werden   App3 ?J ?J2 ?x *  ->  J ?x ?y,  J2 ?y *  
           erzeugt und dann wird ?y zu Wildcard weil unconstrained durch lhs)

           [|  J x y  ;  metamap J xs ys  |]  ==> metamap J (Cons x xs) (Cons y ys)
           erzeugt die Abhaengigkeit
             metamap ?J * *  ->  ?J * *
             (reflexive Abhaengigkeiten wie  metamap ?J * * -> metamap ?J * *
             werden direkt weggelassen)

           [|  base_iso: A isoto B via f |] ==> (curry_iso base_iso): A isoto B via f
           erzeugt die Abhaengigkeit
             (curry_iso ?base_iso): ?A isoto ?B via ?f  ->  ?base_iso: ?A isoto ?B via ?f
             BEACHTE: (curry_iso ?base_iso) wird hier nicht zu Wildcard, obwohl
               ?base_iso Variable ist


           Etwas komplizierter bei sowas wie:
              zipApp [] [] []
              [| J x y  ;  zipApp Js xs ys |]  ==> zipApp (Cons J Js) (Cons x xs) (Cons y ys)
           Man wuerde eigentlich
              zipApp [J_1, .., J_n] * * -> J_i    fuer alle i:{1,..,n}
           wollen, aber das wirkt zu kompliziert und das generische Prinzip "Abhaengigkeit zu 
           Judgments an Positionen nach einem regulaeren Schema in einer Termstruktur" dahinter
           ist wohl nicht ohne Spezialbehandlung zu kriegen.
           Vermutlich einfach die Abhaengigkeitsbeziehungen fuer jede Klausel zu einem Judgement
           erstmal unabhaengig generieren und dann lhs-Unifizierbarkeit unter 
           Nutzung von Wildcards fordern. Wenn das nicht moeglich ist ohne das die rhs ein Wildcard
           enthaelt, wie das bei zipApp der Fall waere, dann Fehlschlag
           "non-uniform dependency across clauses".
           Aber diese Marker-Pattern sind hier dann wohl eine Ausnahme weil man die Klauseln mit
           nicht unifbaren Marker-Pattern als exklusiv ansieht und bei endgueltigen
           Benutzung eines Judgements mit Markern-Pattern immer eine vollstaendige Instantiierung
           des Marker-Patterns vorliegt also die Klauselmenge klar ist.


        Wenn man alles so genau trackt sollte man subsumption
        auf Abhaengigkeiten machen, dh allgemeinere Kanten absorbieren
        speziellere. Sonst wird der Graph u.U. riesig.


        Bei partiellen Applikationen ergeben sich allgemeinere Abhaengigkeiten
          Ueber  App J x  hat man also volle Abhaengigkeit
          auf J statt nur auf J x ?? Bei obiger exakter Analyse
          waere die Abhaengigkeit exakt auf  J x
        Was ist mit Markern?
          Ueber  App (J mark) x  hat man Abhaengigkeit auf  J mark x
          mit konkretem mark. Passt also.
        Was ist mit lokalen brules/frules (ueber Annahmen in brules)?
          Vermutlich einfach statisch zum globalen Abhaengigkeitsgraph
          beitragen lassen bzw. Fehler melden wenn er sich durch
          lokale Regeln veraendert.

        Kann man das dependency tracking  frule -> frule
        jetzt mit dem dependency tracking  judgement -> judgement
        verschmelzen?

        BEACHTE: wenn ich comp. rules wieder anmache muss man die
        Patterns im Abhaengigkeitsgraph auch entsprechend normalisieren
        (ggf. voll neu checken bei neuer comp. rule)



    * !!!!!! RADIKALE VEREINFACHUNG:
      Dependency Analyse einfach rauswerfen und der
      Benutzer soll explizite Phaseneinteilung und Reihenfolge angeben ????
      Mindestens mal ausprobieren wie nervig das waere bei der Animation der
      nonfree datatype Metatheory.

      Letztendlich muss man sich ja sowieso explizite Gedanken ueber die Phaseneinteilung
      machen (zB verschiedene Marker paramisoto, curry_iso, curry_iso_with_iter),
      also ist das vllt doch die richtige Idee das komplett extra-logisch zu machen.

      Zusaetzlich zur Reihenfolge der Phasen sollte auch das verzoegerte Triggern
      ueber Praesenz von Fakten eines Judgements, wie bei Collector-Judgements, moeglich sein.
      Dann braucht man keine Trigger fuer Collector-Judgements mehr.

      Ein bisschen Komfort: benannte Phase, Regeln nutzen Phasennamen zur Zuordnung in ihre Phase,
        seperate Vergabe der Reihenfolge der benannten Phasen

      Etwas bloed ist vllt das man jetzt tatsaechlich selber sicherstellen muss das die
      relevanten Fakten eines Judgements in einer Phase vor der Phase des zugehoerigen
      Collector-Judgements generiert werden.
        Weiterhin ist Wiederverwendung von forward rules dann weniger einfach moeglich
      als was mit hoeherstufigem dependency tracking moeglich gewesent waere. Aber 
      die wiederverwendbare Funktionalitaet laesst sich sowieso besser in algorithmische
      Regelsysteme packen, ggf mit lokalen impliziten forward rules.

      Gut ist das solche Ueberlegungen wie "Regel erst aktiv wenn die
      Hornklauseln-Interpretation bewiesen wurde" jetzt in einer Phasenannotation
      explizit gemacht sind.
        
    * ideen fuer hoeherstufige Judgements:
      * bottomuprew [-[-,+], [-, +]] macht aus J mit Semantik J x y == (x = y)
        eine bottom-up rewriting engine bottomuprew J
      * term-traversierungs-strategien: map_atoms
      * lookup von elementaren Fakten eines Judgements die konjunktiv in P stehen:  lookup J P
          interessant waere hier auch mit Soft-Types zu etablieren das
          P tatsaechlich nur eine Konjunktion darstellt. Aber ist wohl in
          der Anwendung schwierig weil man gerne Quantoren-Unrolling per rewriting nutzt um
          die Konjunktion zu erzeugen. Aber koennte man schon ueber Spezial-Rewrite
          machen...
    * dependency analyse nicht nur Judgements sondern konkrete Judgement-Applikationen 
      bzw Judgement zusammen mit ausgewaehlte Argumente nutzen lassen, also einen Termgraph nutzen 




    * wenn das Goal die Form t elabto t' : B und alle Constraints die Form x_i : A_i(x1 .. x_i-1)
      mit Unif-Variablen x_i (x_i not in t) haben, wobei (:) ein Typisierungsjudgement
      mit dependent product ist, dann wollen wir spaeter das implizite dependent product
        (impllam x_1 .. x_n. t') : PIimpl x1:A1 .. x_n:A(x1 .. x_n-1). B(x1 .. x_n)
      bilden. Hauptsaechliche Schwierigkeit dabei ist die Constraints in eine lineare Reihenfolge
      zu bringen sodass spaeter Constraints x_i : A_i in A_i nur die Unifvar enthalten duerfen
      die in vorherigen Constraints typisiert wurden. (Pientka - Insider's look at LF type inference
      schneidet das kurz an)
      Semantisch fuehrt man dann lambdas ein die implizit bleiben und zeigt
        (impllam x_1 .. x_n. t') : PIimpl x1:A1 ... x_n:A_n(x1 .. x_n-1). B(x1 .. x_n)
      ueber
        !! x_1 ... x_n. x1:A1 ==> .. ==> x_n:A_n(x1 .. x_n-1) ==> t'(x_1 .. x_n) : B(x_1 .. x_n)




  * soft-typisierte hoeherstufige Unifikationsvariablen (mit delay von non-pattern unifications)
    um Typinferenz a la Twelf in alg. Regelsystemen nachbilden zu koennen.
    Vermutlich ist es besser ein Judgment  unifvar A X  zu haben mit input A, output X das
    X als Unifikationsvariable des Typs A deklariert.
    Die Unifikationen wohl auch besser erstmal ganz explizit als  unify t1 t2  judgement
    das zunaechst mal die synthetisierten Typen von t1, t2 unifiziert, dann t1 und t2 selbst. Vermutlich
    sollte man auch das Typsynthesejudgement fuer Terme mit Unifikationvariablen vorgeben.



  * solve_prems umbenennen und aufteilen in solve_goal und solve_goals und dass so aus
    dem toplevel metarec Aufruf nutzen
  * metarec auf mehrere Judgement-Goals um Typinferenz mehrerer Propositionen
    mit Sharing von Variablen zu ermoeglichen
  * verzoegerte non-pattern Unifikationen doch besser vor Constraint-Vereinfachung durchfuehren,
    damit Vereinfachung auf den erfolgten Instantiierungen durchgefuehrt wird.
    Positiver Einfluss von Unifikationen waehrend der Constraint-Vereinfachung auf die
    Problem-Komplexitaet der verzoegerten non-pattern Unifikationen ist ja recht unwahrscheinlich.
    Beste Loesung waere wohl vor und nach der Loesung der verzoegerten Unifikationsproblemen
    die Constraints zu minimieren.



  * constraint simp procs die eine constraint simp rule dynamisch (auf Basis
     aller aktuell vorliegenden Constraints) generieren
     (ggf. unter Modifikation des Kontext, insbes Instantiierungen),
     die dann einmal ausgefuehrt wird
     (und entstehende Constraints werden normal hinzugefuegt)

  * verallgemeinertes non-konfluentes bounded Narrowing zur Constraint-Vereinfachung
    * Das bloede an Narrowing im Vgl zu CHRs ist das Narrowing nur funktioniert wenn mindestens
      ein Constraint eine lokal-eindeutige Loesung hat (d.h. Loesung dieses Constraints ist eindeutig,
      auch wenn man die anderen Constraints ausser Acht laesst).
      Nicht-konfluentes nicht-terminierendes verallgemeinertes Narrowing,
      also bounded, nicht-determ, mit Eindeutigkeitscheck und
      mit mehrkoepfigen Propagierungsregeln (?) waere wohl das coolste.
    * Nicht-Termination durch Laengen-Bound umgehen bis zu dem lokale Eindeutigkeit der
      moeglichen Reduktionsfolgen klar sein muss
    * Unfailing higher-order Knuth-Bendix wichtig, vllt sogar mit HO strukturellem matching statt
      HO pattern matching, zur Bestimmung der elementaren Narrowing-Reduktionsregeln
      (damit dann sogar gewoehnlicheres konfluentes Narrowing) die unter Kongruenzabschluss
      statt nur Substitutionsabschluss angewandt werden sollen?
      Ggf. sogar verallgemeinern auf metarec-conditional equivalence relations statt Gleichungen.
        Narrowing entspricht Paramodulation mit *orientierten* Gleichungen, also Knuth-Bendix Completion
      nur fuer terminierende Orientierung mit Symmetriebildung wichtig fuer Narrowing, weil Narrowing
      selbst die Gleichungsinstanzen fuer kritischen Paare ueber Unifikation findet.
    * Auch sinnvoll fuer allgemeine Unifikation statt nur Constraint-Vereinfachung?!?
      Unifikations-Algorithmen funktionieren ja auch immer dann am schoensten wenn sich
      kanonische Loesungssubstitution ergibt, also erscheint es sinnvoll aehnlich wie
      bei der Nutzung von Narrowing bei lokal-eindeutiger Constraint-Vereinfachung vorzugehen.
        Aber wie realisiert man Netze oder Substitution-Trees modulo Narrowing?


  * resolution modulo metarec
    * parametrisiert auf Mengensynthesejudgement (zumindest auf Universen-Granularitaet)
      von elaborierten Termen um ZF-Lambdas,Produkte waehrend pattern unification
      bilden zu koennen
    * discharge/simp von durch Unifikation instantiierte Constraints
      (aber fuer Universenlevel Constraints ist nichts zusaetzlich zu tun, weil
       das immer Variablen sind und die ueblichen Simp/Propagierungsregeln immer aktiv sind)
    * braucht zusaetzlichen Discharge von Constraints an "hidden" Variablen: 
      * triv discharge von constraints an "hidden" Universenlevel
        (die nicht in Proposition und nicht in Constraints zusammen mit
        anderen nonhidden Variablen vorkommen)
        und gemaess Universenlevel-Ungleichungs-Graph u<=-initial bzw. u<=,u<-terminal sind
        (wobei hier Kanten von hidden Universenleveln nicht gezaehlt werden),
        durch Instantiierung Term ueber 0,max,succ,Universenlevel hochgezaehlt
        gemaess i u<= j, i u< j Kanten
          Universenlevel die <-initial sind im Universenlevel-Ungleichungs-Graph koennte
        man theoretisch auch dischargen wenn man stattdessen n <=u i constraints
        fuer konstantes n nutzt (und zulaesst). Aber nervig und wo ist der Nutzen?
          Koennte uns das zum Problem werden wenn zuviele initiale Level im Beweis aber
        nicht in Proposition vorkommen die sich also anhaeufen?
        klingt unwahrscheinlich -> Fallstudie
      ? discharge von Typing Constraints an "hidden" Typ-Variablen ??
        (die nicht in Proposition und nicht in Constraints zusammen mit
         anderen nonhidden Variablen vorkommen)
           A <: ... -> guniv i  ~~>  A := (% ... . 0)
        Wie kann das vorkommen?
      ? discharge von typing constraints an hidden Term-Variablen ??
           x <: A  ~~>  x := SOME x. x : A   fuer  nonempty A
        Aber sowieso unwahrscheinlich das hidden Term-Variablen relevant sind
        und nonemptiness-Tracking ist typtheoretisch schlecht zu motivieren.
    * als Optimierung: Unifikation verschiedener Universenlevel zwischen denen
      (transitiv) keine Constraints bestehen. Vermutlich reicht auch schon die
      Unifikation von Universenlevel an die garkeine Constraints bestehen






  
*)



(* DONE


    * lokale forward rules (ALREADY DONE, aber ein bisschen anders?)
      Annahmen (von Premissen der Regeln) die selbst Regeln sind (dh insgesamt ist die
      Regel >=3 stufige HHF) als lokale implizte frules auffassen
      und lokale Faktenmenge verwalten, wobei lokale Fakten (ausgewaehlter Judgements?)
      als brules ohne Premissen mit maximaler Prioritaet aufgefasst werden
      * das ist also Hauptsaechlich ne Modifikation von add_assm
      * die lokale Fakten -> brule Konvertierung einfach ueber entsprechende
        globale implizite frules machen die brules mit max Prioritaet generieren:
            J ts ==> maxpriobrule ( J ts )
        Aber vllt besser nicht das gleiche Judgement J wiederverwenden?
      * LAME weil fuer Annahmen ueber Atome quasi ja das Default sein soll direkt
        eine lokale brule ohne Premissen zu generieren,
        also besser Annahmen als lokale brules interpretieren und
        in fact(...) bzw frule(...) gewrappte Annahmen entsprechend als Fakten
        bzw implizite lokale frules 
      * ACHTUNG: lokale implizite frules erfordern genaueres Tracking ob
        man gerade expl. frules anwendet oder nicht sonst werden die lokalen
        Resultate mit lthy declarations eingefuegt!!!
      * ggf auch erlauben solche Annahmen als brules aufzufassen indem man sie
        in brule(...) wrappt
      * was ist wenn aus lokalen Fakten entstandene brules ueberlappen?
        Fehler oder "dynamische Bindung", dh die letzthinzugefuegte brule hat
        hoeherer Prio (was ja wohl bei meiner Impl direkt so sein wuerde)
      * lokale brules brauchen schnelleres gen_add_rule ohne Checks,
        was auch keine dependencies updatet (unnoetig weil nur implizite frules aktiv
        die keine Premissen haben duerfen)
      * statt lokalen schematische Variablen in lokalen brules werden lokale Quantoren genutzt,
        und alle vorkommenden schematischen Variablen und Typvariablen muessen im Kontext 
        der Regel available sein

    * Verallgemeinerte Constraint-Handling rules um Constraints zu loesen und zu minimieren:
        biete multi-headed Propagierungsregeln und backward-Simplifizierungsregeln
        mit metarec-Premissen (inbesondere Unifikation) an
      Ausfuehrung:
        * starte mit den initialen Constraints C_0,
        * bauen einen Hypergraph auf der angibt welche Constraints durch welche anderen
          impliziert werden mit Beweisterm dazu
        * finde Teilmenge von C_0 die ueber die Implikationen im Hypergraph die
          Gesamtmenge C_0 impliziert. Sich wechselseitig-implizierende Constraints in C_0
          bleiben dabei erhalten

  * metarec-basierte Typinferenz als Isabelle-Termelaborationsphase registrieren
    (Constraints ueber spezielle Implikationen nach aussen kommunizieren)
  * Isabelle Kontextprimitiven ueberladen um die Constraints der metarec-basierten
    Typinferenz als Annahmen im Kontext zu interpretieren

  * Elaboration von patterns fuer metarec Klauseln
    (somewhat related to elaboration of antiquotations in
    Christiansen - type directed elaboration of quasiquotations?)
  * Error-Message rewriting nuetzlich? Muesste eigentlich conditional rewriting
    mit metarec Premissen sein. Schwierig mit CHRs?
    (cf. Christiansen - type directed elaboration of quasiquotations)

  * universe level performance/algebraism tradeoff
       annotating PRODs with universe levels speeds up type synthesis
       (which is important for zf unification):
         * no introduction of new universe levels and constraints for products them necessary,
         * type synthesis of universes generates constraint anyway, if we don't
           adopt at least i+n algebraic universe levels (see below),
           but type synthesis for universes is rare if we have a direct universe
           synthesis rule for univlvl-annotated products etc., as universes are mostly under
           type constructors such as sums, products.
             Problem is: if we ever intend to use local type inference we will have
           to make the user put in holes for the univlvl-annotations.
         * we need a checking judgement
         * we have to introduce much more universe levels during elaboration
           (still seems practical, but needs advanced hidden universe level discharge
           and u<=-increasing tree contraction)
             An interesting simple performance improvement (which sacrifices some, hopefully
           nonessential, generality of inferred universe levels) is the idea of sharing
           universe levels along PRODu telescopes. Corresponds to contracting
           some u<=-increasing trees if universe level for product is not used
           
           I.e. if we introduce a new PRODu over x:A, we check the body type B(x) and if it is
           of the form (PRODu k y:A2(x). B'(x,y)) we reuse k and therefore produce
           (PRODu k x:A. PRODu k y:A2(x). B'(x,y)) with the additional constraint
              i u<= k,  where i is given by  A synthty (guniv i),
           on k.


       Note that annotating guniv with a target universe level does not work,
       because because k in   guniv i j synthty guniv j k  is still unknown.
       Annotating with the target universe (not the level) also would not work:
         guniv i U synthty U
       because U would have to be of the form guniv j U' again, so a finite
       guniv term could not even be constructed.

       The problem with algebraic universe levels on the other hand is
       the complexity of their unification and the overspecification by using
       umax instead of the actually necessary constraints. And transforming a term with
       algebraic universe levels to a "univ-generalized" term with non-algebraic
       ones generates new coarser constraints for them, so employing such
       a transformation during unification makes no sense, because the transformed
       term is not equal to the original.
       We could apply the transformation at the latest possible moment, i.e.
       when the two guniv expressions that are to be supunified, but we are mostly interested
       in unification, not supunification. Would work like this:

            explode_univlvl i i' := (i u<= i')

            [|  explode_univlvl i i'  ;  freshFOunifvar k  ;  constraint (i' u< k)  |] ==>
            explode_univlvl (Suc i) k

            [|  explode_univlvl i i' ; explode_univlvl j j'
               freshFOunifvar k  ;  constraint (i' u<= k)  ;  constraint (j' u<= k)  |] ==>
            explode_univlvl (i umax j) k


            [|  explode_univlvl i i'  ; explode_univlvl j j'  ; primunify i' j'  |] ==>
            supunify (guniv i) (guniv j) (guniv i')


       Specialized unification of algebraic universes can work as constraint simplification
       process instead if we allow disjunctive constraints, which leads to a disjunctive
       set of constraint graphs.
       Note that solution is not unique anyway. (Sozeau accumulates constraints with umax?)
       If we want to unify i, j, we first push i+n formation downward with 
       the rewrite rule
         (umax i1 i2) + n = umax (i1 + n) (i2 + n)
       and then we start constraint simplification with the constraints
         i u<= j,  j u<= i
       that have to become completely simplified with the following simplification CHRs
          [| constraint_conj (i1 u<= j, i2 u<= j)  |] ==> umax i1 i2 u<= j
          [| constraint_disj (i u<= j1, i u<= j2)  |] ==> i u<= umax j1 j2
          [| constraint_conj (i1 u< j, i2 u< j)  |] ==> umax i1 i2 u< j
          [| constraint_disj (i u< j1, i u< j2)  |] ==> i u< umax j1 j2
       and the CHRs below for i+n universe level simplification are needed as well.

         Examples
              umax { i, j+2 } = k  --> i u<= k,  j+2 u<= k,  ( k u<= i  OR  k u<= j+2 )
                 == (i u<= k, j+2 u<= k, k u<= i) OR (i u<= k, j+2 u<= k, k u<= j+2)
                 -->  unify k i   OR  unify k (j+2)


         Only allowing algebraic universe levels of the form i+n
       (n>0 from guniv type synthesis) and treating max-formation (from products)
       via constraints would work, because { i+n | i : univlvl, n : nat } is
       closed under instantiation.
       Unification can then stay free of the complexities of max universe levels,
       and it only has to syntactically unify universe levels i+n, j+m with i,j variables,
       leading to an instantiation i := j+(m-n) if m >= n, or j := i+(n-m) otherwise.
       Constraint simplification employs new optional simplification CHRs:
           constraint( i u<= j ) ==> (Suc i) u<= (Suc j)
           constraint( i u< j ) ==> (Suc i) u< (Suc j)
           constraint( i u< j ) ==> (Suc i) u<= j
       and new essential simp CHR
          [| synleq j i  ;  universe_inconsistency |]  ==>  i u< j
          [| synless j i  ;  universe_inconsistency |]  ==>  i u<= j
          universe_inconsistency  ==>  i u< j &&& j u< (Suc i)
          primunify i j  ==>  i u<= j &&& j u< (Suc i)
          primunify j (Suc i)  ==>  i u< j &&& j u<= (Suc i)
       and new essential propagation CHRs
           [| synleq j j'  ;  i u<= j &&& j' u<= k |] ==> i u<= k
           [| synleq j j'  ;  i u<= j &&& j' u< k |] ==> i u< k
           [| synleq j j'  ;  i u< j &&& j' u<= k |] ==> i u< k
           [| synleq j j'  ;  i u< j &&& j' u< k |] ==> i u< k
       under clauses
           synleq i i
           synleq i j ==> synleq i (Suc j)
           synleq i j ==> synleq (Suc i) (Suc j)

           synleq i j ==> synless i (Suc j)

       example universe inconsistency derivations:
             k < i,  i+3 < k   -->   i+3 < i  --> univ_inconsis
             k < i,  i+3 < j+1,  j+2 < k    -->   k < j+1, i+3 < k  --> k < k  --> univ_inconsis
             i+2 < k, k < i+3  --> univ_inconsis
             i+2 < k, k < i+4  --> ok
      
       
       alternatively enumerate all possibly useful i < i+1 facts
       (that are propagated to i u<= i+1 and combined with j u<= i, i u<= k
       facts of course to form full chain)
       with propagation rules (hackish and seems slow, but faster exact
       constraint indexing and first-head-prematching could make this actually
       faster than the approach with explicit syntactic leq, especially since
       large n in universe levels i+n are rare ;
       don't display downwardchain constraints to user for debugging purposes):
            i u<= j ==> downwardchain i &&& downwardchain j
            i u< j ==> downwardchain i &&& downwardchain j
            downwardchain (Suc i) ==> downwardchain i &&& i u< (Suc i)
       also needs new simp CHRs
          universe_inconsistency  ==>  i u< j &&& j u< (Suc i)
          primunify i j  ==>  i u<= j &&& j u< (Suc i)
          primunify j (Suc i)  ==>  i u< j &&& j u<= (Suc i)
           
       example universe inconsistency derivations:
             k < i,  i+3 < k   -->   i+2 < i+3  -->  i+1 < i+2 --> i < i+1, i+1 < i+3, 
               -->  i < i+3, i < i+2  -->  k < k
             k < i,  i+3 < j+1,  j+2 < k    -->  i+2 < i+3, j+1 < j+2, j < j+1
             -->  i+1 < i+2, j < j+2 --> i < i+1, i+2 < i+3, --> i < i+3 ... -->
             k < j+1, j+1 < k  --> k < k 
             i+2 < k, k < i+3  --> univ_inconsis
             i+2 < k, k < i+4  --> ok



       Also we have to use a hack such as
         (PRODu k x:A. B(x)) := (PROD x:A. B(x)) Int (guniv (k-1))
         i <: univlvl := i : nat /\ i > 0
       to enable an easy rule (provable via closure of guniv k under subsets
         k synthty univlvl ==> (PRODu k x:A. B(x)) synthty guniv k,
       otherwise we have to synthesize the universes of the components anyway. 
*)
